{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook to diagnose in order to be able to log compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Roma\\.virtualenvs\\code_gen-n2ugPVn-\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Roma\\.virtualenvs\\code_gen-n2ugPVn-\\lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import pickle\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example, tokenizer, seq_length):\n",
    "    return tokenizer(\n",
    "        example['text'],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=seq_length,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(example):\n",
    "    example['label'] = example['input_ids']\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data saved in create_dataset_save.py module\n",
    "with open(\"../data/dataset_hf_train_val.pkl\", \"rb\") as f:\n",
    "    # Read the data from the file\n",
    "    data_prepped = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"Salesforce/codegen-350M-mono\"\n",
    "device = \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_chrf_score(preds, tokenizer, metric):\n",
    "    logits = preds.predictions[0]\n",
    "    preds_tok = np.argmax(logits, axis=1)\n",
    "    acts = preds.label_ids\n",
    "\n",
    "    decode_predictions = tokenizer.batch_decode(preds_tok,\n",
    "                                                skip_special_tokens=True)\n",
    "    decode_labels = tokenizer.batch_decode(acts, skip_special_tokens=True)\n",
    "\n",
    "    res = metric.compute(predictions=decode_predictions,\n",
    "                         references=decode_labels)\n",
    "    return {'chrf_score': res['score']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 40\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/157 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 157/157 [00:00<00:00, 463.10 examples/s]\n",
      "Map: 100%|██████████| 53/53 [00:00<00:00, 499.16 examples/s]\n",
      "Map: 100%|██████████| 157/157 [00:00<00:00, 3730.21 examples/s]\n",
      "Map: 100%|██████████| 53/53 [00:00<00:00, 3312.25 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenize_partial = partial(tokenize_function,\n",
    "                               tokenizer=tokenizer,\n",
    "                               seq_length=seq_length)\n",
    "\n",
    "tokenized_dataset = data_prepped.map(tokenize_partial,\n",
    "                                batched=True,\n",
    "                                batch_size=batch_size).map(\n",
    "                                    add_labels,\n",
    "                                    batched=True,\n",
    "                                    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrf = evaluate.load(\"chrf\")\n",
    "compute_metric_partial = partial(compute_chrf_score, tokenizer = tokenizer, metric = chrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.scriptrun = azureml.core.script_run:ScriptRun._from_run_dto with exception (packaging 23.1 (c:\\users\\roma\\.virtualenvs\\code_gen-n2ugpvn-\\lib\\site-packages), Requirement.parse('packaging<=23.0,>=20.0')).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (packaging 23.1 (c:\\users\\roma\\.virtualenvs\\code_gen-n2ugpvn-\\lib\\site-packages), Requirement.parse('packaging<=23.0,>=20.0'), {'azureml-core'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.ReusedStepRun = azureml.pipeline.core.run:StepRun._from_reused_dto with exception (packaging 23.1 (c:\\users\\roma\\.virtualenvs\\code_gen-n2ugpvn-\\lib\\site-packages), Requirement.parse('packaging<=23.0,>=20.0'), {'azureml-core'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.StepRun = azureml.pipeline.core.run:StepRun._from_dto with exception (packaging 23.1 (c:\\users\\roma\\.virtualenvs\\code_gen-n2ugpvn-\\lib\\site-packages), Requirement.parse('packaging<=23.0,>=20.0'), {'azureml-core'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (numpy 1.26.0 (c:\\users\\roma\\.virtualenvs\\code_gen-n2ugpvn-\\lib\\site-packages), Requirement.parse('numpy!=1.19.4,<1.24; sys_platform == \"win32\"'), {'azureml-dataset-runtime'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint hyperdrive = azureml.train.hyperdrive:HyperDriveRun._from_run_dto with exception (packaging 23.1 (c:\\users\\roma\\.virtualenvs\\code_gen-n2ugpvn-\\lib\\site-packages), Requirement.parse('packaging<=23.0,>=20.0'), {'azureml-core'}).\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]You're using a CodeGenTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting `use_cache=False`...\n",
      "                                               \n",
      " 33%|███▎      | 20/60 [02:56<04:18,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.313001871109009, 'eval_chrf_score': 0.029652109121347678, 'eval_runtime': 39.6936, 'eval_samples_per_second': 1.335, 'eval_steps_per_second': 0.176, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 67%|██████▋   | 40/60 [06:55<02:09,  6.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.269188165664673, 'eval_chrf_score': 0.028982331097937207, 'eval_runtime': 98.8618, 'eval_samples_per_second': 0.536, 'eval_steps_per_second': 0.071, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|██████████| 60/60 [09:41<00:00,  9.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8674826622009277, 'eval_chrf_score': 0.03242280617316204, 'eval_runtime': 22.967, 'eval_samples_per_second': 2.308, 'eval_steps_per_second': 0.305, 'epoch': 3.0}\n",
      "{'train_runtime': 582.1429, 'train_samples_per_second': 0.809, 'train_steps_per_second': 0.103, 'train_loss': 1.4203641255696615, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60, training_loss=1.4203641255696615, metrics={'train_runtime': 582.1429, 'train_samples_per_second': 0.809, 'train_steps_per_second': 0.103, 'train_loss': 1.4203641255696615, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data collator - Assembles data into batches for training\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(output_dir='../trainer_res',\n",
    "                                  gradient_checkpointing=True,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  num_train_epochs=3)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metric_partial,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code_gen-n2ugPVn-",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
